\documentclass[letterpaper,12pt]{article}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath, amssymb}  % improve math presentation
\usepackage{graphicx} % takes care of graphics including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyperlinks inside the generated pdf file
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
\usepackage{blindtext}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{standalone}
%++++++++++++++++++++++++++++++++++++++++


\begin{document}

\title{Signals}
\author{Harsh Agrawal}
\date{\today}
\maketitle

\begin{abstract}
    This notebook compiles content from Signals lectures delivered by Prof.\ Dario Farina as part of the BIOE50005 Module (2nd Year Imperial MBE). A total of 9 lectures are covered in these notes. Please suggest any relevant changes/improvements at \href{ha1822@ic.ac.uk}{ha1822@ic.ac.uk}.
\end{abstract}


\subsection*{Introduction and Types of Signals}

\textit{Mathematically}, signals are functions of one or more independent variables. Eg: The instantaneous velocity of a moving body at every given time interval can be represented as a signal: \(v(t)\). Based on the type of input, there are two types of signals:
\begin{enumerate}
    \item \textbf{Continuous-time}: Signals where the domain of the input variable is continuous. An analog signal is a continuous-time signal. The notation used for continuous-time signals is:

          \[x(t), \quad t \in \mathbb{R}\]

    \item \textbf{Discrete-time}: Signals where the domain of the input signal is not a continuum and belongs to a discrete set. Discrete-time signals can be generated by sampling points from a continuous signal. The commonly used notation for discrete-time signals is:

          \[x[n], \quad n \in \mathbb{Z}\]

          Discrete-time signals can also be interpreted as follows:
          \begin{itemize}
              \item Sequence of numbers: A signal $x[n]$ can be interpreted as a sequence of numbers $x[n]$ where n is the index of each number.

                    \[x = \{x[n]\}, \quad -\infty < n < \infty\]

              \item Vector: A signal $s[n]$ with defined over an input set of length $x$ can be interpreted as a vector of $x$ dimensions.

          \end{itemize}

\end{enumerate}

\begin{figure}
    \centering
    \includestandalone[width=0.8\textwidth]{figures/sin_test}%     without .tex extension
    % or use \input{mytikz}
    \caption{Sampling of a continuous signal $f(t) = \sin(t)$}\label{fig:sampling_signal}
\end{figure}

\subsubsection*{Digital Signals \& Quantization}

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/quantized_sin}%     without .tex extension
    % or use \input{mytikz}
    \caption{Quantization of $\sin$ wave. This figure isn't a mathematically rigorous depiction as the quantization levels might not be equally spaced.}\label{fig:sampling_signal_quantized}
\end{figure}

\textbf{Digital Signal}: A discrete time signal that is (often) sampled from a continuous analog signal such as figure\ref{fig:sampling_signal_quantized}. Digital signals are also often quantized. \\
\textbf{Quantization}: When sampling a continuous signal, the amplitude of the signal is rounded to the nearest quantized level.

\textit{Quantization always implies \textbf{losing information} as the exact amplitude of the wave is lost while sampling at a given time interval.}

\subsubsection*{Other Types of Signals}
Depending on the property of prediction of signal beforehand, there are two categories of signals:
\begin{enumerate}
    \item \textbf{Deterministic}: These signals can be predicted exactly, and before they've been observed. This implies that a known mathematical formulation for the signal is already present. Eg: If we know a signal follows the function \(x(t) = \sin(2{\pi}t)\), we already know all information about the signal.
    \item \textbf{Stochastic}: These signals can't be predicted before they are observed. These signals convey new information when observed.  Eg: EEG measurement, etc. If we already knew the analytical formulation of these signals then we wouldn't be required to observe these signals.
\end{enumerate}

\textbf{Periodic Signals}: A signal is termed periodic if it follows this property:

\begin{equation}
    x(t) = x(t + T) \quad \forall t
    \label{eq:periodic_signal_condition}
\end{equation}

The signal should follow this for all values of $t$ (\textit{$\forall$ represents `for all`}). Here, $T$ = Period of the signal. Similarly, for a discrete signal: \(x[n] = x[n + N]\).

\subsection*{Signal Energy \& Power}
For a continuous-time signal, $x(t)$ for $t_1 \le t \le t_2$, The Energy (E) of the signal and Power (P) are defined as:

\begin{equation}
    \label{eq:cont_time_energy}
    E\ =\ \int_{t_{1}}^{t_{2}}\left|x\left(t\right)\right|^{2}\cdot dt
\end{equation}

\begin{equation}
    \label{eq:cont_time_power}
    P\ =\ \frac{1}{t_{2}-t_{1}}\int_{t_{1}}^{t_{2}}\left|x\left(t\right)\right|^{2}\cdot dt
\end{equation}

Similarly, for a discrete-time signal $x[n]$ for $n_1 \le n \le n_2$, The Energy (E) of the signal and Power (P) are defined as:

\begin{equation}
    \label{eq:disc_time_energy}
    E\ =\ \sum_{n=n_{1}}^{n_{2}}\left|x\left[n\right]\right|^{2}
\end{equation}

\begin{equation}
    \label{eq:disc_time_power}
    P\ =\ \frac{1}{n_{2}-n_{1}+1}\sum_{n=n_{1}}^{n_{2}}\left|x\left[n\right]\right|^{2}
\end{equation}

These definitions hold when $-\infty < t < \infty$ or $-\infty < n < \infty$

\subsection*{Complex Signals}
\textit{Before understanding complex signals, it's helpful to recap complex numbers.}

The complex plane is quite useful when dealing with two dimensions: the real dimension (x-axis) and the imaginary dimension (y-axis). Complex numbers can be represented in multiple ways:
\begin{itemize}
    \item \textbf{Vector Form}: \(\Vec{z} = x + j\cdot y\) where $j=\sqrt{-1}$. The magnitude of this vector is given by:

          \[|\Vec{z}|^2 = \sqrt{x^2 + y^2}\]

          \begin{figure}[!ht]
              \centering
              \includestandalone[width=0.5\textwidth]{figures/vector}%     without .tex extension
              % or use \input{mytikz}
              \caption{Vector representation of the generic complex number \(\Vec{z} = x + j\cdot y\).}\label{fig:signal_vector_graph}
          \end{figure}

          \begin{align*}
              \theta & = \text{Phase of the complex number} \\
              x      & = |z|\cos(\theta)                    \\
              y      & = |z|\sin(\theta)                    \\
          \end{align*}

          By putting the values of x and y into the vector form of complex numbers, we can obtain:

          \[\Vec{z} = x + j\cdot y = |z|(\cos(\theta) + j\cdot \sin(\theta))\]

    \item \textbf{Exponential Form}: We can use Euler's relation, \(e^{j\cdot \theta} = \cos(\theta) + j\cdot \sin(\theta)\), to obtain the following equality from the above equation:

          \begin{equation}
              \Vec{z} = x + j\cdot y = |z|(\cos(\theta) + j\cdot \sin(\theta)) = |z|\cdot e^{j\cdot \theta}
              \label{eq:euler_complex_no}
          \end{equation}


\end{itemize}

\subsection*{Examples}
\subsubsection*{Proof of a Periodic Signal}
We can use the condition for periodicity we defined above (eqn.\ref{eq:periodic_signal_condition}) to prove whether a given signal is periodic.

Let's use the complex exponential as an example:

\[x(t) = e^{j\cdot \omega_0 \cdot t}\]

This equation is periodic only if \(x(t) = x(t + T)\), thus, the following needs to be true:

\begin{equation*}
    \begin{aligned}
        e^{j\cdot \omega_0 \cdot t} & = e^{j\cdot \omega_0 \cdot (t + T)}                                \\
                                    & = e^{(j\cdot \omega_0 \cdot t) + (j\cdot \omega_0 \cdot T)}        \\
                                    & = e^{(j\cdot \omega_0 \cdot t)}\cdot e^{(j\cdot \omega_0 \cdot T)}
    \end{aligned}
\end{equation*}

We know from Euler, that:

\[e^{(j\cdot \omega_0 \cdot T)} = \cos(\omega_0\cdot T) + j\cdot \sin(\omega_0\cdot T)\]

To ensure LHS=RHS, and prove that the above signal is periodic, we need to find a value of $\omega_0\cdot t$ such that $e^{j\cdot \omega_0 \cdot T} = 1$. Cleverly, if we set $\omega_0\cdot T = \pm2\pi$, then $\cos(\omega_0\cdot T) = 1$, $\sin(\omega_0\cdot T) = 0$, and ultimately, $e^{j\cdot \omega_0 \cdot T} = 1$.

To set a generic condition for periodicity:

\[T\ =\ \frac{2\pi}{\left|\omega_{0}\right|}\]

\textit{The reason we only take the absolute value of $\omega_0$ is because T is always positive and only $\omega_0$ can take negative values.}

A similar is true for discrete-time signals:

\[N = \frac{2\pi}{|\omega_0|}\]

However, here N can only be an integer, thus $\omega_0$ also needs to be a fraction of $2\pi$ or:

\[\omega_0 = \frac{2\pi}{k}, \quad k \in Integer\]

This also tells us that in the discrete-time domain, frequencies can't be arbitrarily increased from $0 \to +\infty$ to increase the rotation speed. They can only be sampled from a given domain. If we go any faster, we won't be able to distinguish the additional speed. Eg: when the wheel of a car goes too fast, we can't identify its speed.

\subsubsection*{Example Calculation of Energy \& Power}
Let's calculate the energy for a complex signal $x(t) = e^{j{\omega_0}t}$  and $\omega_0$ = angular frequency for the range $-\infty < t < \infty$. We can use the mathematical definition of energy for a continuous-time signal from eq. \ref{eq:cont_time_energy} as follows:

\[\text{Energy}\ =\ \int_{-\infty}^{\infty}\left|x\left(t\right)\right|^{2}\cdot dt\]

Using Euler's relation, we can separately evaluate $|x(t)|^2$:

\[|x(t)|^{2}=|e^{j\omega_{0}t}|^{2}\]

Complex numbers have the following property:

\[|z|^2=|z|\cdot|\Bar{z}|; \quad \Bar{z}\text{: complex conjugate } (a-j\cdot b)\]

Expanding $e^{j\omega_{0}t}$ using Euler's relation and the above property:

\begin{equation*}
    \begin{aligned}
        e^{j\omega_{0}t} & = (\cos(\omega_0 \cdot t) + j\cdot \sin(\omega_0 \cdot t)) \cdot (\cos(\omega_0 \cdot t) - j\cdot \sin(\omega_0 \cdot t)) \\
                         & = \cos^2(\omega_0 \cdot t) - j^2\cdot \sin^2(\omega_0 \cdot t)                                                            \\
                         & = \cos^2(\omega_0 \cdot t) + \sin^2(\omega_0 \cdot t)                                                                     \\
                         & = 1
    \end{aligned}
\end{equation*}

Plugging this into the integral above, we obtain:

\[E\ =\ \int_{-\infty}^{+\infty}1\cdot dt=\left[t\right]_{-\infty}^{+\infty}=+\infty\]

We can say that complex exponentials are \textbf{signals with infinity energy}. We can also calculate the power of the complex exponential:
\begin{equation*}
    \begin{aligned}
        \text{Power} & = \lim_{T \to +\infty} \frac{1}{2T}\int_{-T}^{+T}\left|x\left(t\right)\right|^{2}\cdot dt \\
                     & = \frac{1}{2T}\cdot[t]^{+T}_{-T}                                                          \\
                     & = \frac{1}{2T}\cdot2T                                                                     \\
                     & = 1
    \end{aligned}
\end{equation*}

From the above, we can say that complex exponentials are \textbf{signals with finite energy and equals to 1}.

\subsection*{Unit Impulse \& Unit Step}
\subsubsection*{Unit Impulse}
In the discrete time domain, the unit impulse is the simplest signal, defined as follows:

\[\delta[n] =
    \begin{cases}
        1, & n = 0    \\
        0, & n \neq 0
    \end{cases}
\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/unit_impulse}
    \caption{A unit impulse, $\delta[n]$ centered at n=0}\label{fig:unit_impulse}
\end{figure}

Similarly, the discrete-time impulse delayed by the integer k is:

\[\delta[n - k] =
    \begin{cases}
        1, & n = k    \\
        0, & n \neq k
    \end{cases}
\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/unit_impulse_off_center}
    \caption{A unit impulse, $\delta[n-k]$ off-centered at k}
    \label{fig:unit_impulse_off_center}
\end{figure}

If we multiply a generic discrete-time signal, $x[n]$, with a unit pulse centered at k, $\delta[n-k]$, we obtain:
\[ x[n]\cdot \delta[n-k] = x[k]\cdot \delta[n-k]\]

The reason this is true is because at $n\neq k$, the value of $\delta[n-k] = 0$. Thus, the output is simply the signal at the time step $n=k$ multiplied by the unit impulse at $n=k$. By definition, the unit impulse = 1 at $n=k$, so the equation now becomes:

\[ x[n]\cdot \delta[n-k] = x[k]\cdot \delta[n-k] = x[k]\]

If we now compute:

\begin{equation*}
    \begin{aligned}
        \sum_{n=-\infty}^{+\infty}x[n]\cdot \delta[n-k] & = \sum_{n=-\infty}^{+\infty}x[k]\cdot \delta[n-k]                        \\
                                                        & = x[k] \cdot \sum_{n=-\infty}^{+\infty}\delta[n-k]                       \\
                                                        & = x[k] \quad \quad ( \because \sum_{n=-\infty}^{+\infty}\delta[n-k] = 1) \\
    \end{aligned}
\end{equation*}

\[\therefore x[k] = \sum_{n=-\infty}^{+\infty} x[n] \cdot \delta[n-k]\]

If we arbitrarily change the variables and assume $n \rightleftharpoons k$, we obtain:

\[x[n] = \sum_{k=-\infty}^{+\infty} x[k] \cdot \delta[k-n]\]

But since the unit pulse = 1 at n=k (or k=n), $\delta[k-n] = \delta[n-k]$. In both situations, when k=n, $\delta[n-k]$ = $\delta[n-k]$ (equal to 1 when n = k, equal to 0 when n $\neq$ k). So:

\begin{equation}
    \label{eq:sum_unit_pulse}
    x[n] = \sum_{k=-\infty}^{+\infty} x[k] \cdot \delta[n-k]
\end{equation}

What this equation intuitively tells us is that you can construct a discrete-time signal by taking the value of that discrete-time signal at different timesteps scaled by a unit impulse off-centered at that time step.

Another paraphrase: \textit{Any arbitrary discrete-time signal can be expressed as the sum of scaled and delayed impulses}.

\newpage
\subsubsection*{Unit Step}
A unit step in the discrete-time domain is defined as:

\[u[n] =
    \begin{cases}
        1, & n \geq 0 \\
        0, & n < 0
    \end{cases}
    = \sum_{k=0}^{+\infty} \delta[n-k]\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/unit_step}
    \caption{A unit step, $u[n]$}\label{fig:unit_step}
\end{figure}

We can also note the following property:

\[\delta [n] = u[n] - u[n-1]\]

This means that the unit impulse is the discrete-time derivative of the unit step. Here, since 1 unit is the smallest increment in x, $dx = 1$, and $dy = u[n] - u[n] - 1$.

Similar to the unit step in the discrete time, we can define a unit step in the continuous time domain as:
\[u(t) =
    \begin{cases}
        1, & t > 0 \\
        0, & t < 0
    \end{cases}\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/unit_step_cont_domain}
    \caption{A unit step, $u(t)$ in continous-time}\label{fig:unit_step_cont_dom}
\end{figure}

As we can see from figure \ref{fig:unit_step_cont_dom}, the unit step is discontinuous at $t=0$. This discontunity is an issue in the continuous time domain as the unit step mentioned above is not \textbf{formally differentiable}.

In order to calculate that we define another signal: $u_{\Delta}(t)$ as follows:

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/unit_step_diff_cont}
    \caption{Modified unit step, $u_{\Delta}(t)$ in continuous-time}\label{fig:unit_step_cont_dom_delta}
\end{figure}

Now if we take the derivative of the above signal, we obtain:
\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/derivative_unit_step_cont}
    \caption{Derivative of modified unit step, $u_{\Delta}(t)$ in continuous-time}\label{fig:derivative_unit_step_cont}
\end{figure}

We can see that the derivative plot resembles a unit impulse ($\therefore du_{\Delta}(t) = \delta_{\Delta}(t)$). However, unlike the unit impulse in discrete time which is of infintesimally small duration, this unit impulse has a duration of size $\Delta$. If $\Delta$ decreases, $\frac{1}{\Delta}$ increases; however, the area under the unit impulse remains constant and equals to 1. This can be mathematical represented by:

\[\int_{-\infty}^{+\infty}\delta_{\Delta}(t)\cdot dt = 1, \quad \forall \Delta\]

We can define the unit impulse in the continuous time domain ($\delta(t)$) as a limit of the unit impulse we obtained from the modified unit step ($\delta_{\Delta}(t)$) as $\Delta \to 0$:

\[\delta(t) = \lim_{\Delta \to 0} \delta_{\Delta}(t) = \begin{cases}
        0,      & \forall t \neq 0 \\
        \neq 0, & t = 0
    \end{cases}\]

\textit{Explaination: As we decrease $\Delta$, $\frac{1}{\Delta}$ continues to increase till the point where $\delta_{\Delta}(t) \approx \infty$ and $t + dt \approx t$.}

By our original definition, the area under the unit impulse is still 1.

\[\int_{-\infty}^{+\infty}\delta(t)\cdot dt = 1\]

This is strange because the integral of a function whose base $\approx$ 0 should be 0 but in our case is 1. That's why the unit impulse we defined above is not a function but a \textbf{distribution} and should be graphically denoted with an arrow instead of a line as follows:

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/unit_step_distribution}
    \caption{Unit Impulse Distribution representation in continous-time domain.}\label{fig:unit_step_distribution}
\end{figure}

\end{document}
