\documentclass[letterpaper,12pt]{article}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath, amssymb, mathtools}  % improve math presentation
\usepackage{graphicx} % takes care of graphics including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage{caption}
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyperlinks inside the generated pdf file
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
\usepackage{blindtext}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{standalone}
\usepackage{xcolor}

\definecolor{darkgray}{gray}{0.3}
% \newcommand{\annot}[1]{{\textcolor{darkgray}{\textit{#1}}}}
\newcommand{\annot}[1]{\textcolor{darkgray}{\textit{#1}}}
\captionsetup[figure]{justification=centering}
%++++++++++++++++++++++++++++++++++++++++

\renewcommand{\footnoterule}{%
  \kern-3pt % space above the rule
  \hrule width \textwidth height 0.4pt
  \kern5pt % space below the rule
}

\begin{document}

\title{Signals}
\author{Harsh Agrawal}
\date{\today}
\maketitle

\begin{abstract}
    This notebook compiles content from Signals lectures delivered by Prof.\ Dario Farina as part of the BIOE50005 Module (2nd Year Imperial MBE). A total of 9 lectures are covered in these notes. Please suggest any relevant changes/improvements at \href{ha1822@ic.ac.uk}{ha1822@ic.ac.uk}.
\end{abstract}


\subsection*{Introduction and Types of Signals}

\textit{Mathematically}, signals are functions of one or more independent variables. Eg: The instantaneous velocity of a moving body at every given time interval can be represented as a signal: \(v(t)\). Based on the type of input, there are two types of signals:
\begin{enumerate}
    \item \textbf{Continuous-time}: Signals where the domain of the input variable is continuous. An analog signal is a continuous-time signal. The notation used for continuous-time signals is:

          \[x(t), \quad t \in \mathbb{R}\]

    \item \textbf{Discrete-time}: Signals where the domain of the input signal is not a continuum and belongs to a discrete set. Discrete-time signals can be generated by sampling points from a continuous signal. The commonly used notation for discrete-time signals is:

          \[x[n], \quad n \in \mathbb{Z}\]

          Discrete-time signals can also be interpreted as follows:
          \begin{itemize}
              \item Sequence of numbers: A signal $x[n]$ can be interpreted as a sequence of numbers $x[n]$ where n is the index of each number.

                    \[x = \{x[n]\}, \quad -\infty < n < \infty\]

              \item Vector: A signal $s[n]$ with defined over an input set of length $x$ can be interpreted as a vector of $x$ dimensions.

          \end{itemize}

\end{enumerate}

\begin{figure}
    \centering
    \includestandalone[width=0.8\textwidth]{figures/lecture_1/sin_test}%     without .tex extension
    % or use \input{mytikz}
    \caption{Sampling of a continuous signal $f(t) = \sin(t)$}\label{fig:sampling_signal}
\end{figure}

\subsubsection*{Digital Signals \& Quantization}

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/lecture_1/quantized_sin}%     without .tex extension
    % or use \input{mytikz}
    \caption{Quantization of $\sin$ wave. This figure isn't a mathematically rigorous depiction as the quantization levels might not be equally spaced.}\label{fig:sampling_signal_quantized}
\end{figure}

\textbf{Digital Signal}: A discrete time signal that is (often) sampled from a continuous analog signal such as figure\ref{fig:sampling_signal_quantized}. Digital signals are also often quantized. \\
\textbf{Quantization}: When sampling a continuous signal, the amplitude of the signal is rounded to the nearest quantized level.

\textit{Quantization always implies \textbf{losing information} as the exact amplitude of the wave is lost while sampling at a given time interval.}

\subsubsection*{Other Types of Signals}
Depending on the property of prediction of signal beforehand, there are two categories of signals:
\begin{enumerate}
    \item \textbf{Deterministic}: These signals can be predicted exactly, and before they've been observed. This implies that a known mathematical formulation for the signal is already present. Eg: If we know a signal follows the function \(x(t) = \sin(2{\pi}t)\), we already know all information about the signal.
    \item \textbf{Stochastic}: These signals can't be predicted before they are observed. These signals convey new information when observed.  Eg: EEG measurement, etc. If we already knew the analytical formulation of these signals then we wouldn't be required to observe these signals.
\end{enumerate}

\textbf{Periodic Signals}: A signal is termed periodic if it follows this property:

\begin{equation}
    x(t) = x(t + T) \quad \forall t
    \label{eq:periodic_signal_condition}
\end{equation}

The signal should follow this for all values of $t$ (\textit{$\forall$ represents `for all`}). Here, $T$ = Period of the signal. Similarly, for a discrete signal: \(x[n] = x[n + N]\).

\subsection*{Signal Energy \& Power}
For a continuous-time signal, $x(t)$ for $t_1 \le t \le t_2$, The Energy (E) of the signal and Power (P) are defined as:

\begin{equation}
    \label{eq:cont_time_energy}
    E\ :=\ \int_{t_{1}}^{t_{2}}\left|x\left(t\right)\right|^{2}\cdot dt
\end{equation}

\begin{equation}
    \label{eq:cont_time_power}
    P\ :=\ \frac{1}{t_{2}-t_{1}}\int_{t_{1}}^{t_{2}}\left|x\left(t\right)\right|^{2}\cdot dt
\end{equation}

Similarly, for a discrete-time signal $x[n]$ for $n_1 \le n \le n_2$, The Energy (E) of the signal and Power (P) are defined as:

\begin{equation}
    \label{eq:disc_time_energy}
    E\ =\ \sum_{n=n_{1}}^{n_{2}}\left|x\left[n\right]\right|^{2}
\end{equation}

\begin{equation}
    \label{eq:disc_time_power}
    P\ =\ \frac{1}{n_{2}-n_{1}+1}\sum_{n=n_{1}}^{n_{2}}\left|x\left[n\right]\right|^{2}
\end{equation}

These definitions hold when $-\infty < t < \infty$ or $-\infty < n < \infty$

\subsection*{Complex Signals}
\textit{Before understanding complex signals, it's helpful to recap complex numbers.}

The complex plane is quite useful when dealing with two dimensions: the real dimension (x-axis) and the imaginary dimension (y-axis). Complex numbers can be represented in multiple ways:
\begin{itemize}
    \item \textbf{Vector Form}: \(\Vec{z} = x + j\cdot y\) where $j=\sqrt{-1}$. The magnitude of this vector is given by:

          \[|\Vec{z}|^2 = \sqrt{x^2 + y^2}\]

          \begin{figure}[!ht]
              \centering
              \includestandalone[width=0.5\textwidth]{figures/lecture_1/vector}%     without .tex extension
              % or use \input{mytikz}
              \caption{Vector representation of the generic complex number \(\Vec{z} = x + j\cdot y\).}\label{fig:signal_vector_graph}
          \end{figure}

          \begin{align*}
              \theta & = \text{Phase of the complex number} \\
              x      & = |z|\cos(\theta)                    \\
              y      & = |z|\sin(\theta)                    \\
          \end{align*}

          By putting the values of x and y into the vector form of complex numbers, we can obtain:

          \[\Vec{z} = x + j\cdot y = |z|(\cos(\theta) + j\cdot \sin(\theta))\]

    \item \textbf{Exponential Form}: We can use Euler's relation, \(e^{j\cdot \theta} = \cos(\theta) + j\cdot \sin(\theta)\), to obtain the following equality from the above equation:

          \begin{equation}
              \Vec{z} = x + j\cdot y = |z|(\cos(\theta) + j\cdot \sin(\theta)) = |z|\cdot e^{j\cdot \theta}
              \label{eq:euler_complex_no}
          \end{equation}


\end{itemize}

\subsection*{Examples}
\subsubsection*{Proof of a Periodic Signal}
We can use the condition for periodicity we defined above (eqn.\ref{eq:periodic_signal_condition}) to prove whether a given signal is periodic.

Let's use the complex exponential as an example:

\[x(t) = e^{j\cdot \omega_0 \cdot t}\]

This equation is periodic only if \(x(t) = x(t + T)\), thus, the following needs to be true:

\begin{equation*}
    \begin{aligned}
        e^{j\cdot \omega_0 \cdot t} & = e^{j\cdot \omega_0 \cdot (t + T)}                                \\
                                    & = e^{(j\cdot \omega_0 \cdot t) + (j\cdot \omega_0 \cdot T)}        \\
                                    & = e^{(j\cdot \omega_0 \cdot t)}\cdot e^{(j\cdot \omega_0 \cdot T)}
    \end{aligned}
\end{equation*}

We know from Euler, that:

\[e^{(j\cdot \omega_0 \cdot T)} = \cos(\omega_0\cdot T) + j\cdot \sin(\omega_0\cdot T)\]

To ensure LHS=RHS, and prove that the above signal is periodic, we need to find a value of $\omega_0\cdot t$ such that $e^{j\cdot \omega_0 \cdot T} = 1$. Cleverly, if we set $\omega_0\cdot T = \pm2\pi$, then $\cos(\omega_0\cdot T) = 1$, $\sin(\omega_0\cdot T) = 0$, and ultimately, $e^{j\cdot \omega_0 \cdot T} = 1$.

To set a generic condition for periodicity:

\[T\ =\ \frac{2\pi}{\left|\omega_{0}\right|}\]

\textit{The reason we only take the absolute value of $\omega_0$ is because T is always positive and only $\omega_0$ can take negative values.}

A similar is true for discrete-time signals:

\[N = \frac{2\pi}{|\omega_0|}\]

However, here N can only be an integer, thus $\omega_0$ also needs to be a fraction of $2\pi$ or:

\[\omega_0 = \frac{2\pi}{k}, \quad k \in Integer\]

This also tells us that in the discrete-time domain, frequencies can't be arbitrarily increased from $0 \to +\infty$ to increase the rotation speed. They can only be sampled from a given domain. If we go any faster, we won't be able to distinguish the additional speed. Eg: when the wheel of a car goes too fast, we can't identify its speed.

\subsubsection*{Example Calculation of Energy \& Power}
Let's calculate the energy for a complex signal $x(t) = e^{j{\omega_0}t}$  and $\omega_0$ = angular frequency for the range $-\infty < t < \infty$. We can use the mathematical definition of energy for a continuous-time signal from eq. \ref{eq:cont_time_energy} as follows:

\[\text{Energy}\ =\ \int_{-\infty}^{\infty}\left|x\left(t\right)\right|^{2}\cdot dt\]

Using Euler's relation, we can separately evaluate $|x(t)|^2$:

\[|x(t)|^{2}=|e^{j\omega_{0}t}|^{2}\]

Complex numbers have the following property:

\[|z|^2=|z|\cdot|\Bar{z}|; \quad \Bar{z}\text{: complex conjugate } (a-j\cdot b)\]

Expanding $e^{j\omega_{0}t}$ using Euler's relation and the above property:

\begin{equation*}
    \begin{aligned}
        e^{j\omega_{0}t} & = (\cos(\omega_0 \cdot t) + j\cdot \sin(\omega_0 \cdot t)) \cdot (\cos(\omega_0 \cdot t) - j\cdot \sin(\omega_0 \cdot t)) \\
                         & = \cos^2(\omega_0 \cdot t) - j^2\cdot \sin^2(\omega_0 \cdot t)                                                            \\
                         & = \cos^2(\omega_0 \cdot t) + \sin^2(\omega_0 \cdot t)                                                                     \\
                         & = 1
    \end{aligned}
\end{equation*}

Plugging this into the integral above, we obtain:

\[E\ =\ \int_{-\infty}^{+\infty}1\cdot dt=\left[t\right]_{-\infty}^{+\infty}=+\infty\]

We can say that complex exponentials are \textbf{signals with infinity energy}. We can also calculate the power of the complex exponential:
\begin{equation*}
    \begin{aligned}
        \text{Power} & = \lim_{T \to +\infty} \frac{1}{2T}\int_{-T}^{+T}\left|x\left(t\right)\right|^{2}\cdot dt \\
                     & = \frac{1}{2T}\cdot[t]^{+T}_{-T}                                                          \\
                     & = \frac{1}{2T}\cdot2T                                                                     \\
                     & = 1
    \end{aligned}
\end{equation*}

From the above, we can say that complex exponentials are \textbf{signals with finite energy and equals to 1}.

\newpage
\subsection*{Unit Impulse \& Unit Step}
\subsubsection*{Unit Impulse}
In the discrete time domain, the unit impulse is the simplest signal, defined as follows:

\[\delta[n] :=
    \begin{cases}
        1, & n = 0    \\
        0, & n \neq 0
    \end{cases}
\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/lecture_1/unit_impulse}
    \caption{A unit impulse, $\delta[n]$ centered at n=0}\label{fig:unit_impulse}
\end{figure}

Similarly, the discrete-time impulse delayed by the integer k is:

\[\delta[n - k] =
    \begin{cases}
        1, & n = k    \\
        0, & n \neq k
    \end{cases}
\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/lecture_1/unit_impulse_off_center}
    \caption{A unit impulse, $\delta[n-k]$ off-centered at k}
    \label{fig:unit_impulse_off_center}
\end{figure}

If we multiply a generic discrete-time signal, $x[n]$, with a unit pulse centered at k, $\delta[n-k]$, we obtain:
\[ x[n]\cdot \delta[n-k] = x[k]\cdot \delta[n-k]\]

The reason this is true is because at $n\neq k$, the value of $\delta[n-k] = 0$. Thus, the output is simply the signal at the time step $n=k$ multiplied by the unit impulse at $n=k$. By definition, the unit impulse = 1 at $n=k$, so the equation now becomes:

\[ x[n]\cdot \delta[n-k] = x[k]\cdot \delta[n-k] = x[k]\]

If we now compute:

\begin{equation*}
    \begin{aligned}
        \sum_{n=-\infty}^{+\infty}x[n]\cdot \delta[n-k] & = \sum_{n=-\infty}^{+\infty}x[k]\cdot \delta[n-k]                        \\
                                                        & = x[k] \cdot \sum_{n=-\infty}^{+\infty}\delta[n-k]                       \\
                                                        & = x[k] \quad \quad ( \because \sum_{n=-\infty}^{+\infty}\delta[n-k] = 1) \\
    \end{aligned}
\end{equation*}

\[\therefore x[k] = \sum_{n=-\infty}^{+\infty} x[n] \cdot \delta[n-k]\]

If we arbitrarily change the variables and assume $n \rightleftharpoons k$, we obtain:

\[x[n] = \sum_{k=-\infty}^{+\infty} x[k] \cdot \delta[k-n]\]

But since the unit pulse = 1 at n=k (or k=n), $\delta[k-n] = \delta[n-k]$. In both situations, when k=n, $\delta[n-k]$ = $\delta[n-k]$ (equal to 1 when n = k, equal to 0 when n $\neq$ k). So:

\begin{equation}
    \label{eq:sum_unit_pulse_discrete_time}
    x[n] = \sum_{k=-\infty}^{+\infty} x[k] \cdot \delta[n-k]
\end{equation}

What this equation intuitively tells us is that you can construct a discrete-time signal by taking the value of that discrete-time signal at different timesteps scaled by a unit impulse off-centered at that time step.

Another paraphrase: \textit{Any arbitrary discrete-time signal can be expressed as the sum of scaled and delayed impulses}.

\newpage
\subsubsection*{Unit Step}
A unit step in the discrete-time domain is defined as:

\[u[n] =
    \begin{cases}
        1, & n \geq 0 \\
        0, & n < 0
    \end{cases}
    = \sum_{k=0}^{+\infty} \delta[n-k]\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.8\textwidth]{figures/lecture_1/unit_step}
    \caption{A unit step, $u[n]$}\label{fig:unit_step}
\end{figure}

We can also note the following property:

\[\delta [n] = u[n] - u[n-1]\]

This means that the unit impulse is the discrete-time derivative of the unit step. Here, since 1 unit is the smallest increment in x, $dx = 1$, and $dy = u[n] - u[n] - 1$.

Similar to the unit step in the discrete time, we can define a unit step in the continuous time domain as:
\[u(t) =
    \begin{cases}
        1, & t > 0 \\
        0, & t < 0
    \end{cases}\]

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/lecture_2/unit_step_cont_domain}
    \caption{A unit step, $u(t)$ in continous-time}\label{fig:unit_step_cont_dom}
\end{figure}

As we can see from figure \ref{fig:unit_step_cont_dom}, the unit step is discontinuous at $t=0$. This discontunity is an issue in the continuous time domain as the unit step mentioned above is not \textbf{formally differentiable}.

In order to calculate that we define another signal: $u_{\Delta}(t)$ as follows:

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/lecture_2/unit_step_diff_cont}
    \caption{Modified unit step, $u_{\Delta}(t)$ in continuous-time}\label{fig:unit_step_cont_dom_delta}
\end{figure}

Now if we take the derivative of the above signal, we obtain:
\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.6\textwidth]{figures/lecture_2/derivative_unit_step_cont}
    \caption{Derivative of modified unit step, $u_{\Delta}(t)$ in continuous-time}\label{fig:derivative_unit_step_cont}
\end{figure}

We can see that the derivative plot resembles a unit impulse ($\therefore du_{\Delta}(t) = \delta_{\Delta}(t)$). However, unlike the unit impulse in discrete time which is of infintesimally small duration, this unit impulse has a duration of size $\Delta$. If $\Delta$ decreases, $\frac{1}{\Delta}$ increases; however, the area under the unit impulse remains constant and equals to 1. This can be mathematical represented by:

\[\int_{-\infty}^{+\infty}\delta_{\Delta}(t)\cdot dt = 1, \quad \forall \Delta\]

We can define the unit impulse in the continuous time domain ($\delta(t)$) as a limit of the unit impulse we obtained from the modified unit step ($\delta_{\Delta}(t)$) as $\Delta \to 0$:

\[\delta(t) = \lim_{\Delta \to 0} \delta_{\Delta}(t) = \begin{cases}
        0,      & \forall t \neq 0 \\
        \neq 0, & t = 0
    \end{cases}\]

\textit{Explaination: As we decrease $\Delta$, $\frac{1}{\Delta}$ continues to increase till the point where $\delta_{\Delta}(t) \approx \infty$ and $t + dt \approx t$.}

By our original definition, the area under the unit impulse is still 1.

\[\int_{-\infty}^{+\infty}\delta(t)\cdot dt = 1\]

This is strange because the integral of a function whose base $\approx$ 0 should be 0 but in our case is 1. That's why the unit impulse we defined above is not a function but a \textbf{distribution} and should be graphically denoted with an arrow instead of a line as follows:

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.5\textwidth]{figures/lecture_2/unit_step_distribution}
    \caption{Unit Impulse Distribution representation in continous-time domain.}\label{fig:unit_step_distribution}
\end{figure}

Similarly, a unit impulse in the continous time domain off-centered at $t=t_{0}$ looks like:

\begin{figure}[!ht]
    \centering
    \includestandalone[width=0.5\textwidth]{figures/lecture_2/unit_impulse_cont_off_centered}
    \caption{Unit Impulse Distribution representation in continous-time domain off-centered at $t=t_{0}$.}\label{fig:unit_impulse_cont_off_centered}
\end{figure}

Now, if we try and obtain the product of the unit impulse, $\delta(t)$, with a generic signal, $x(t)$, in continous time, we can first calculate the product of the generic signal with our modified unit impulse, $\delta_{\Delta}(t)$, in continous time:

\[x(t)\cdot \delta_{\Delta}(t) \approx x(0)\cdot \delta_{\Delta}(t)\]

\textit{This is because the modified unit impulse is only non-zero at the region of $\Delta$ which is $\approx 0$}. To convert the modified unit impulse to a standard unit impulse, we can take the limit $\Delta \to 0$:

\[\lim_{\Delta \to 0} x(t) \cdot \delta_{\Delta}(t) = x(0) \cdot \lim_{\Delta \to 0} \delta_{\Delta}(t) = x(0) \cdot \delta(t)\]

Here, the amplitude of the $\delta(t)$ is infinite, but the product (or area) is still = 1. Mathematically,

\begin{equation*}
    \begin{aligned}
        \int_{t=-\infty}^{+\infty} x(0) \cdot \delta(t) \cdot dt & = x(0) \cdot \int_{t=-\infty}^{+\infty} \delta(t) \cdot dt \\
                                                                 & = x(0)                                                     \\
    \end{aligned}
\end{equation*}

\textit{Note: Even though the amplitude of $\delta(t)$ is infinite, when multiplied with a generic signal and integrated gives the area = 1 as per our definition.}

\vspace{0.5cm}
Interestingly, we have come to the same conclusion for the product of unit impulse with a generic signal in the continous time domain as we did in the discrete time domain.
\[x[n] \cdot \delta[n] = x[0] \cdot \delta[n], \quad \text{(Discrete Time)}\]
\[x(t) \cdot \delta(t) = x(0) \cdot \delta(t), \quad \text{(Continuos Time)}\]

We can continue the generalization for the product of a generic signal with a unit impulse off-centered at $t=t_{0}$ in the continous time domain:

\[x(t) \cdot \delta(t-t_{0}) = x(t_{0}) \cdot \delta(t-t_{0})\]

We evaluate the integral of the above equation with respect to $t$ utilizing the relation derived above:

\begin{align*}
    \int_{t=-\infty}^{+\infty} x(t) \cdot \delta(t-t_{0})\cdot dt & = \int_{t=-\infty}^{+\infty} x(t_{0}) \cdot \delta(t-t_{0})\cdot dt \\
                                                                  & =x(t_{0})\int_{t=-\infty}^{+\infty} \delta(t-t_{0})\cdot dt         \\
    \intertext{\annot{Since $\delta(t-t_0) = 1$ at $t=t_{0}$, the integral evaluates to 1.}}
                                                                  & = x(t_{0})
    \\
    \annot{Substituting $t_{0}=t$, and $t=\tau$,}
    \\
    x(t)                                                          & = \int_{-\infty}^{+\infty} x(\tau) \cdot \delta(\tau-t)\cdot d\tau  \\
                                                                  & \annot{*Note: $ (\tau-t) = (t-\tau)$}
\end{align*}

Similar to eq.~\ref{eq:sum_unit_pulse_discrete_time}, we now have a relation for the continous time domain:

\begin{equation}
    \label{eq:sum_unit_pulse_cont_time}
    x(t) = \int_{\tau=-\infty}^{+\infty} x(\tau) \cdot \delta(\tau-t)\cdot d\tau
\end{equation}

\subsection*{Convolution}
We define \textbf{convolution}\footnote{*Note: Convolution $\neq$ Multiplication} or \textbf{convolutional product} as a new operator defined by the symbol `$*$' between two generic signals as:

\begin{equation}
    x(t) * h(t) := \int_{-\infty}^{+\infty} x(\tau) \cdot h(t-\tau)\cdot d\tau \quad \text{(Continous Time)}
    \label{eq:convolution_continous_time}
\end{equation}

\begin{equation}
    x[n] * h[n] := \sum_{-\infty}^{+\infty} x[k] \cdot h[t-k] \quad \text{(Discrete Time)}
    \label{eq:convolution_discrete_time}
\end{equation}

Let's utilizing eq.~\ref{eq:sum_unit_pulse_cont_time} and eq.~\ref{eq:sum_unit_pulse_discrete_time}, and our defintion of convolution, we can obtain the convolutional product of a generic signal with a unit impulse for both discrete and continous time domains:

\begin{equation}
    x(t) * \delta(t) = x(t)
    \label{eq:convolution_unit_impulse_cont_time}
\end{equation}

\begin{equation}
    x[n] * \delta[n] = x[n]
    \label{eq:convolution_unit_impulse_discrete_time}
\end{equation}

By extension for arbitrary delays (k for discrete time, $t_{0}$ for continous time), we have:

\begin{equation}
    x(t - t_{0}) = x(t) * \delta(t - t_{0})
    \label{eq:convolution_unit_impulse_cont_time_delayed}
\end{equation}

\begin{equation}
    x[n - k] = x[n] * \delta[n - k]
    \label{eq:convolution_unit_impulse_discrete_time_delayed}
\end{equation}

\subsection*{Scalar Product and Norm of Signals}

Before we define the scalar product and norm for signals, let's recap the definition of scalar product and norm for vectors.

We know that for two arbitrary 3-D vectors $\Vec{A}$ \& $\Vec{B}$, their scalar product is given by:

\[\Vec{A} \cdot \Vec{B} = A_{x}\cdot B_{x} + A_{y}\cdot B_{y} + A_{z}\cdot B_{z}\]

Moreover, the projection of $\Vec{A}$ in x-axis $A_{x}$ is also obtained by the scalar product of $\Vec{A}$ with the unit vector $\Vec{i}$ in the x-axis:

\[A_{x} = \Vec{A}\cdot \Vec{i}\]

\textbf{Norm-2}: The norm-2 of the vector is defined as the euclidean length (or magnitude) of the vector.
\[\| \Vec{A} \|_2 = \sqrt{\Vec{A} \cdot \Vec{A}} = \sqrt{A_{x}^2 + A_{y}^2 + A_{z}^2}\]

We can thus extend the definition of scalar product as follows:
\[\Vec{A}\cdot\Vec{B} = \| \Vec{A} \|_2 \cdot \| \Vec{B} \|_2\ \cdot \cos(\theta) \quad \text{\annot{\{$\theta$ = angle between $\Vec{A}$ and $\Vec{B}$\}}}\]

We can similarly define the scalar product between two discrete-time signals using the notation: $\langle \cdot \rangle$ as:

\begin{equation}
    \label{eq:scalar_product_discrete_time}
    \langle x_{1}[n], x_{2}[n] \rangle = \sum_{n=-\infty}^{+\infty} x_{1}[n] \cdot x_{2}^{*}[n]
\end{equation}

% TODO: Add explaination for the complex conjugate
\textit{\textbf{Note:} The $*$ in the above equation (above $x_{2}$) denotes the complex conjugate of the signal. For real signals, the complex conjugate is the same as the original signal. See the supplementary information section on`Magnitude of the Vector' for explaination.}

\vspace{0.5cm}

\subsubsection*{Scalar Product and Norm for Discrete-Time Signals}

We can also define the \textbf{norm-2} of a discrete-time signal as:

\begin{equation}
    \begin{aligned}
        \| x[n] \|_2 & = \sqrt{\langle x[n], x[n] \rangle}                                                                         \\
                     & = \sqrt{\sum_{n=-\infty}^{+\infty} x[n] \cdot x^{*}[n]}                                                     \\
                     & = \sqrt{\sum_{n=-\infty}^{+\infty}|x[n]|^{2}}                                                               \\
                     & = \sqrt{\text{Energy}}                                  & \annot{\text{From eq.~\ref{eq:disc_time_energy}}}
    \end{aligned}
    \label{eq:norm_discrete_time}
\end{equation}

Norm-2 is another way to characterize the intensity of a signal.\textit{ Dividing the signal by its norm-2 implies normalizing the signal to its norm-2.}

We can further extend the definition of norm-2 to \textbf{norm-p}  for a discrete-time signal as follows:

\begin{equation}
    \label{eq:norm_p_discrete_time}
    \| x[n] \|_p := {(\sum_{n=-\infty}^{+\infty} |x[n]|^{p})}^{\frac{1}{p}} \quad 1 \leq p < \infty
\end{equation}

For example, the norm-1 of a discrete-time signal is given by:

\[\| x[n] \|_1 = \sum_{n=-\infty}^{+\infty} |x[n]| \]

The infinite or max norm is given by:

\[\| x[n] \|_\infty = \underset{n}{\text{max}} |x[n]| \]

\subsubsection*{Scalar Product and Norm for Continuos-Time Signals}

Similar to eq.~\ref{eq:scalar_product_discrete_time}, we can define the scalar product between two continous-time signals as:
\begin{equation}
    \label{eq:scalar_product_cont_time}
    \langle x_{1}(t), x_{2}(t) \rangle := \int_{t=-\infty}^{+\infty} x_{1}(t) \cdot x_{2}^{*}(t) \cdot dt
\end{equation}

We can also define the \textbf{norm-2} of a continous-time signal as:
\begin{equation}
    \label{eq:norm_2_cont_time}
    \| x(t) \|_2 = \sqrt{\langle x(t), x(t) \rangle} = \sqrt{\int_{t=-\infty}^{+\infty} x(t) \cdot x^{*}(t) \cdot dt}
\end{equation}

And \textbf{norm-p} (1 $\leq$ p) as:
\begin{equation}
    \label{eq:norm_p_cont_time}
    \| x(t) \|_p = {(\int_{t=-\infty}^{+\infty} |x(t)|^{p} \cdot dt)}^{\frac{1}{p}}
\end{equation}

\subsection*{Similarity Between Signals}
In case of vectors, we can measure there similarity by obtaining the cosine of the angle between the vectors.
\[\cos(\phi) = \frac{\Vec{A} \cdot \Vec{B}}{A \cdot B}\]

If we have two identical vectors, the cosine of the angle between them would be 1. Similarily, if we have two orthogonal vectors, the cosine of the angle between them would be 0.

We can extend this concept to signals by defining the \textbf{similarity} between two signals as their normalized scalar product:
\[\frac{\langle x_1(t), x_2(t) \rangle}{\| x_1(t) \|_2 \cdot \| x_2(t) \|_2}\quad \text{(Continous Time)}\]
\[\frac{\langle x_1[n], x_2[n] \rangle}{\| x_1[n] \|_2 \cdot \| x_2[n] \|_2}\quad \text{(Discrete Time)}\]

TODO:ADD INFO ABOUT CROSS-CORRELATION FUNCTION AND MSE.

\subsection*{Orthonormal Basis Functions}
We are quite familiar with the concept of Orthonormal basis functions. For example, 3-d vectors can be expressed as a linear combination of the 3 orthonormal vectors: $\Vec{i}$, $\Vec{j}$, $\Vec{k}$.
These Orthonormal vectors (or basis functions) have the following property:

\[\langle \Vec{i} \cdot \Vec{j} \rangle = 0\]
\[\langle \Vec{i} \cdot \Vec{i} \rangle = 1\]

We previously obtained a similar results for signal deconstruction into a linear sum of unit impulses:

\begin{align*}
    x[n]                  & = \sum_{k=-\infty}^{+\infty} x[k] \cdot \delta [n-k]                   \\ \\
    \annot{Where, }                                                                                \\
    \annot{$x[n]$}        & = \annot{Discrete Time Signal}                                         \\
    \annot{$x[k]$}        & = \annot{Projection of signal x into the basis function $\delta[n-k]$} \\
    \annot{$\delta[n-k]$} & = \annot{An orthonormal basis function.}
\end{align*}

Here, we can call $\delta[n-k]$ an orthonormal basis function because it satisfies both of the criterias we listed above:
\[\delta[n-k_{1}] \cdot \delta[n-k_{2}] = 0\]
\[\delta[n-k] \cdot \delta[n-k] = 1\]

This is true because a unit impulse is only non-zero at one value and thus at either $n=k_{1}$ or $n=k_{2}$, the delta function = 0.

Simply put, the above relation for discrete time signals is the projection of the signal into it's orthonormal basis functions of infinite dimensions (\textit{assuming the the original signal is of infinite length}).

Following, the above we can define a new set of orthonormal basis functions such that:
\[{\{\varphi_{i}(t)\}}^{N}_{i=1} = \langle \varphi_{i}(t), \varphi_{k}(t)\rangle = \begin{cases}
        0, & i \neq k \\
        1, & i = k
    \end{cases}
\]

Now, to say that a given signal belongs to the space of these basis functions, we should be able to completely describe the signal as a linear combination of these basis functions. For example, a signal $x(t)$ can be expressed as:

\begin{align*}
    x(t)              & = \sum_{i=1}^{N} a_{i} \cdot \varphi_{i} \\
    \annot{And, } a_k & = \langle x(t), \varphi_{k}(t)\rangle
\end{align*}

Here, $\varphi_{k}(t)$ is one of the set of orthonormal basis functions and $a_{k}$ is the projection of the signal $x(t)$ into the basis function $\varphi_{k}(t)$ or the co-efficient of multiplication. $a_{k}$ can be be both real or complex.

\textit{Note: The reason we obtain the co-efficient by taking the dot product with that basis function is to ensure that everything equates to 0 except for that particular signal basis function dimension $x_{k}(t)$. This is only possible if the basis functions are \textbf{orthonormal}.}

Similar to vectors $a_{i} \cdot \varphi_{i}(t)$ gives the best approximation of $x(t)$ in the space of the basis functions $\varphi_{i}(t)$. For an arbitrary n-d signal, its best approximation in, let's say, any given 3 dimensions will be:

\[{x}_{3d} = a_{1}\cdot \varphi_{1}(t) + a_{2}\cdot \varphi_{2}(t) + a_{3}\cdot \varphi_{3}(t)\]

Kindly checkout the Additional Information section on `Digital Modulation using Orthonormal Basis Functions' for a practical example of orthonormal basis functions.

\subsection*{Fourier Basis functions}
We define a special set of orthnormal basis functions to encompass periodic functions. We call them the fourier basis functions and mathematically define them as follows:

\begin{align*}
    \varphi_{k}(t)            & = \frac{1}{\sqrt{T}}\cdot e^{j\frac{2\pi}{T}k t} \quad \forall \; 0 \leq t < T \\
    \annot{Where, }                                                                                            \\
    \annot{$j$}               & = \annot{$\sqrt{-1}$}                                                          \\
    \annot{$e$}               & = \annot{Complex exponential function}                                         \\
    \annot{$T$}               & = \annot{Period of the function}
    \\
    \annot{$\frac{2 \pi}{T}$} & = \omega_{0} = \annot{Fundamental frequency of the function}
    \\
    \annot{$k$}               & = \annot{Harmonic number or multiplicative factor. $k \in\mathbb{Z}$}
\end{align*}

Here, we can deduce the following properties of the fourier basis functions:

\begin{itemize}
    \item Each basis function is complex exponential (harmonic) oscilating with multiple of the fundamental frequency, $\omega_{0}$.
    \item As the Fundamental frequency is increased, the Period of the function decreases.
    \item This infinite set of functions is orthonormal for the interval $0 \leq t < T$.
\end{itemize}

\subsubsection*{Proof of Orthonormality of Fourier Basis Functions}
To demonstrate orthonormality, we need to prove:
\[\langle \varphi_{i}(t), \varphi_{k}(t) \rangle = 0, \quad i\neq k \]

\[\langle \varphi_{i}(t), \varphi_{k}(t) \rangle = 1, \quad i = k \]

By the defintion of scalar product of signals defined above, we can write:
\begin{align*}
    \langle \varphi_{i}(t), \varphi_{k}(t) \rangle                  & = \int_{t=0}^{T} \varphi_{i}(t) \cdot \varphi_{k}^{*}(t) \cdot dt                                                     \\
                                                                    & = \int_{0}^{T}(\frac{1}{\sqrt{T}} e^{j\frac{2\pi}{T}i t}) \cdot (\frac{1}{\sqrt{T}} e^{-j\frac{2\pi}{T}k t}) \cdot dt \\
                                                                    & = \frac{1}{T} \int_{0}^{T} e^{j\frac{2\pi}{T}(i-k) t} \cdot dt                                                        \\
    \annot{\{As, $\int e^{ax}\cdot dx = \frac{1}{a}\cdot e^{ax}$\}} &                                                                                                                       \\
                                                                    & =  [\frac{1}{T}\cdot\frac{1}{j\cdot\frac{2\pi}{T}\cdot (i-k)}]\cdot {[e^{j (\frac{2\pi}{t}) (i-k)t}]}^{T}_{0}         \\
    \annot{At $t=T$:}                                               &                                                                                                                       \\
    \annot{$e^{j \frac{2 \pi}{T} (i-k) T}$}                         & = \annot{$e^{j 2\pi(i-k)}$}                                                                                           \\
                                                                    & = \annot{$\cos(2\pi(i-k)) + j\cdot \sin(2\pi(i-k))$}                                                                  \\
                                                                    & = \annot{$1 \quad \{\because \sin(2\pi n) = 0, \cos(2\pi n) = 1 \quad \forall n \in \mathbb{Z}\}$}                    \\
    \annot{At $t=0$:}                                               &                                                                                                                       \\
    \annot{$e^{j \frac{2 \pi}{T} (i-k) 0}$}                         & = \annot{$1$}                                                                                                         \\ \\
    \annot{Plugging back to the integral,}                          &                                                                                                                       \\
    \langle \varphi_{i}(t), \varphi_{k}(t) \rangle                  & = [\frac{1}{T}\cdot\frac{1}{j\cdot\frac{2\pi}{T}\cdot (i-k)}]\cdot (1 - 1)                                            \\
                                                                    & = 0
\end{align*}

\newpage
\subsection*{Properties of Fourier Transforms}
\subsubsection*{Property of Duality}

The Duality Property tells us that if $x(t)$ has a Fourier Transform $X(\omega)$, then if we form a new function of time that has the functional form of the transform, $X(t)$, it will have a Fourier Transform $x(\omega)$ that has the functional form of the original time function (but is a function of frequency) as described mathematically below\footnote{Taken from https://class.ece.uw.edu/235dl/EE235/Project/lesson16/lesson16.html}. The Duality property is very useful because it can enable to solve Fourier Transforms that would be difficult to compute directly.

\[ \text{If, } \mathcal{F}\{x(t)\} = X(j\omega)\]
\[ \text{then, } \mathcal{F}\{X(t)\} = 2\pi\cdot x(-\omega)\]

For example, last time we calculated that the Fourier Transform of a unit impulse = 1 ($\mathcal{F}\{\delta(t)\}\ = 1$). We can use the duality property to calculate the Fourier Transform of a constant function, $X(t) = 1$:

\begin{equation*}
    \begin{aligned}
        \mathcal{F}\{1\} & = 2\pi\cdot\delta(-\omega)                                                         \\
                         & = 2\pi\cdot\delta(\omega) \quad [\text{$\because \delta(t) = 0$ apart from $t=0$}]
    \end{aligned}
\end{equation*}

\textit{Check another example of the application of the dual property for $x(t) = \delta(t + t_{0})$ in the supplementary notes.}

\subsubsection*{Proof of the Duality Property}
\textbf{Shift to Additional Information too until you find a good proof.}
We know from the definition of the duality property the following relation:

\[\mathcal{F}\{x(t)\} = X(j\omega) \quad \rightarrow \quad \mathcal{F}\{X(t)\} = 2\pi\cdot x(-\omega)\]

The prove the dual relation, we can calculate the Inverse Fourier Transform of the $2\pi\cdot x(-\omega)$ term:

\begin{equation*}
    \begin{aligned}
        \mathcal{F}^{-1}\{2\pi\cdot x(-\omega)\} & = \frac{1}{2\pi}\int_{-\infty}^{+\infty}2\pi\cdot x\left(-\omega\right)\cdot e^{j\omega t}\cdot d\omega                                                \\
                                                 & = \frac{2\pi}{2\pi}\int_{-\infty}^{+\infty}\cdot x\left(-\omega\right)\cdot e^{j\omega t}\cdot d\omega                                                 \\
        \annot{Let, $u = -\omega$ and $du=-d\omega$}                                                                                                                                                      \\
                                                 & = 1+ 1                                                                                                  & \annot{Let, $u = -\omega$ and $du=-d\omega$}
    \end{aligned}
\end{equation*}

\newpage
\section*{Additional Information}
\subsubsection*{Magnitude of a Vector}\label{sec:magnitude_vector}

Let's take a generic complex number, $z = a + jb$. The complex conjugate of this complex number is given by:

\[z^{*} = a - jb\]

If we obtain the product between the complex number and its complex conjugate, we obtain:

\begin{align*}
    z\cdot z^{*} & = (a + jb)\cdot (a - jb) \\
                 & = a^2 + b^2              \\
                 & = |z|^2
    \label{eq:complex_conjugate}
\end{align*}

Thus, the use of the complex conjugate is essential in obtaining the magnitude of a complex number because as you can notice, the product of z by itself would not result in $a^2 + b^2$.
\end{document}
